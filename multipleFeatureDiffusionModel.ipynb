{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from collections import deque\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.metrics import structural_similarity as ssim, peak_signal_noise_ratio as psnr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create numpy grid \n",
    "In order to enable image representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### FOR ALL COLUMNS \n",
    "\n",
    "# Check for GPU availability\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Read the CSV file with coordinates and features\n",
    "df = pd.read_csv('/home/s184310/3.Project/data/final_merged_data/50_all_data.csv')\n",
    "\n",
    "# Constants for cell sizes in degrees (approximations)\n",
    "cell_size_50m_lat = 0.000450  # Approx. 50m in latitude\n",
    "cell_size_50m_lon = 0.000799  # Approx. 50m in longitude at Copenhagen's latitude\n",
    "\n",
    "# Specify the bounding box (Nyhavn and Kongens Nytorv area with slight increase)\n",
    "copenhagen_bbox = [55.545, 12.175, 55.809, 12.745]\n",
    "\n",
    "# Generate grid cells and their centers\n",
    "def generate_grid(min_lat, min_lon, max_lat, max_lon, cell_size_lat, cell_size_lon):\n",
    "    lat_points = np.arange(min_lat, max_lat, cell_size_lat)\n",
    "    lon_points = np.arange(min_lon, max_lon, cell_size_lon)\n",
    "    centers = []\n",
    "    for lat in lat_points:\n",
    "        for lon in lon_points:\n",
    "            center_lat = lat + cell_size_lat / 2\n",
    "            center_lon = lon + cell_size_lon / 2\n",
    "            centers.append((center_lat, center_lon))\n",
    "    return centers\n",
    "\n",
    "centers = generate_grid(*copenhagen_bbox, cell_size_50m_lat, cell_size_50m_lon)\n",
    "\n",
    "# Convert centers to PyTorch tensor and move to device\n",
    "centers_tensor = torch.tensor(centers, dtype=torch.float32).to(device)\n",
    "\n",
    "# Map coordinates to the nearest center point\n",
    "def find_nearest_center(lat, lon, centers):\n",
    "    lat_lon_tensor = torch.tensor([lat, lon], dtype=torch.float32).to(device)\n",
    "    distances = torch.sqrt(torch.sum((centers - lat_lon_tensor) ** 2, dim=1))\n",
    "    nearest_center_idx = torch.argmin(distances)\n",
    "    return nearest_center_idx.item()  # Convert to Python int\n",
    "\n",
    "# Adding a progress bar to the mapping process\n",
    "print(\"Mapping coordinates to the nearest center point...\")\n",
    "df['Center_Index'] = df.progress_apply(lambda row: find_nearest_center(row['Latitude'], row['Longitude'], centers_tensor), axis=1)\n",
    "\n",
    "# Create a numpy array with the same structure as the grid\n",
    "num_lat_cells = int((copenhagen_bbox[2] - copenhagen_bbox[0]) / cell_size_50m_lat) + 1\n",
    "num_lon_cells = int((copenhagen_bbox[3] - copenhagen_bbox[1]) / cell_size_50m_lon) + 1\n",
    "\n",
    "# Determine the number of features (excluding Latitude and Longitude)\n",
    "num_features = df.shape[1] - 3  # Exclude Latitude, Longitude, and Center_Index\n",
    "\n",
    "# Initialize a 3D array to hold the features for each cell\n",
    "grid_array = np.full((num_lat_cells, num_lon_cells, num_features), np.nan)  # Initialize with NaNs\n",
    "\n",
    "# Fill the numpy array with feature values\n",
    "print(\"Filling the numpy array with feature values...\")\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    center_idx = int(row['Center_Index'])  # Ensure center_idx is an integer\n",
    "    lat_idx = center_idx // num_lon_cells\n",
    "    lon_idx = center_idx % num_lon_cells\n",
    "    for feature_idx in range(num_features):\n",
    "        if np.isnan(grid_array[lat_idx, lon_idx, feature_idx]):\n",
    "            grid_array[lat_idx, lon_idx, feature_idx] = row.iloc[2 + feature_idx]  # Offset by 2 to skip Latitude and Longitude columns\n",
    "\n",
    "# Save the numpy array to a file\n",
    "np.save('/home/s184310/3.Project/data/final_merged_data/50_all_data.npy', grid_array)\n",
    "\n",
    "print(\"Grid array saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the numpy array from the file (for later use)\n",
    "loaded_grid_array = np.load('/home/s184310/3.Project/data/final_merged_data/50_all_data.npy')\n",
    "\n",
    "# Print the shape of the loaded grid array\n",
    "print(\"Shape of loaded grid array:\", loaded_grid_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "project_path = \"/home/s184310/3.Project/data/final_merged_data\" #\"/home/s184310/3.Project\" the path to ypur project root\n",
    "patch_size = 50 # The size of the patches to use (You will probably need to experiment with this parameter)\n",
    "patch_stride = 10 # The stride used when generating data\n",
    "channels = 51 # The number of channels in the data\n",
    "BATCH_SIZE = 4\n",
    "#define the area we use as our test data\n",
    "rect_x_start, rect_x_end = 100, 290 \n",
    "rect_y_start, rect_y_end = 200, 390\n",
    "\n",
    "channel_to_display =0 #the channel to show in the plots (0-39)\n",
    "\n",
    "test_size=0.2\n",
    "random_state=42\n",
    "\n",
    "mask_count = 140\n",
    "T = 1000 #Timesteps\n",
    "\n",
    "\n",
    "epochs = 350 # Try more!\n",
    "learning_rate = 0.001\n",
    "\n",
    "num_images = 5 #number of images shown in plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_array = np.load(f'{project_path}/50_all_data.npy')\n",
    "print(grid_array.shape)\n",
    "# Initialize a list to store the count of valid data points for each channel\n",
    "valid_data_counts = []\n",
    "\n",
    "# Iterate over each channel\n",
    "for channel in range(grid_array.shape[-1]):\n",
    "    # Extract the data for the current channel\n",
    "    channel_data = grid_array[:, :, channel]\n",
    "    \n",
    "    # Count the number of valid data points (not 0 and not -1)\n",
    "    valid_count = np.sum((channel_data != 0) & (channel_data != -1))\n",
    "    \n",
    "    # Append the count to the list\n",
    "    valid_data_counts.append(valid_count)\n",
    "\n",
    "# Find the channel with the maximum valid data points\n",
    "max_valid_channel = np.argmax(valid_data_counts)\n",
    "max_valid_count = valid_data_counts[max_valid_channel]\n",
    "\n",
    "print(f\"Channel with the most valid data: {max_valid_channel}\")\n",
    "print(f\"Number of valid data points in this channel: {max_valid_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/s184310/3.Project/data/final_merged_data/50_all_data.csv')\n",
    "\n",
    "# Get the column names\n",
    "columns = df.columns\n",
    "\n",
    "# Calculate the actual column index (accounting for starting from the third column)\n",
    "column_index = channel_to_display+2\n",
    "\n",
    "# Ensure the index is within the range of the dataframe's columns\n",
    "if column_index < len(columns):\n",
    "    column_name = columns[column_index]\n",
    "    print(f\"The column to be displayed is: {column_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of column indices starting from the second column\n",
    "column_indices = list(range(2, len(columns)))\n",
    "\n",
    "# Print the column names along with their indices\n",
    "for i in column_indices:\n",
    "    print(f\"Column {i-2}: {columns[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the grid array and select the channel to display\n",
    "#grid_array = np.load(f'{project_path}/50m_26_features_array.npy')\n",
    "##print(\"Shape of loaded grid array:\", grid_array.shape)\n",
    "channel_to_display = 33\n",
    "\n",
    "# Update all -1 values to -2\n",
    "grid_array[grid_array == -1] = -2\n",
    "\n",
    "# Mask the -2 values by setting them to NaN for visualization\n",
    "#channel_data = np.where(grid_array[:, :, channel_to_display] == -2, np.nan, grid_array[:, :, channel_to_display])\n",
    "channel_data = np.where(grid_array[:, :, channel_to_display] == 0, np.nan, grid_array[:, :, channel_to_display])\n",
    "#channel_data = grid_array[:, :, channel_to_display]\n",
    "\n",
    "# Mask the -1 values by setting them to NaN\n",
    "channel_data = np.where(channel_data == -2, np.nan, channel_data)\n",
    "channel_data = np.where(channel_data == 0, np.nan, channel_data)\n",
    "\n",
    "# Load the dataframe to get column names\n",
    "#df = pd.read_csv('/home/s184310/3.Project/data/merged_mapi_poi_metrics.csv')\n",
    "columns = df.columns\n",
    "\n",
    "# Calculate the actual column index (accounting for starting from the third column)\n",
    "column_index = channel_to_display + 2\n",
    "\n",
    "# Ensure the index is within the range of the dataframe's columns\n",
    "if column_index < len(columns):\n",
    "    column_name = columns[column_index]\n",
    "    title = f\"Original data for {column_name} feature\"\n",
    "else:\n",
    "    title = f\"Original data for channel {channel_to_display}: Column index out of range\"\n",
    "\n",
    "# Plot the data with a perceptually uniform colormap\n",
    "plt.figure(figsize=(10, 8))  # Increase figure size for better coverage visibility\n",
    "plt.imshow(channel_data, cmap='viridis', interpolation='nearest', aspect='auto')\n",
    "plt.colorbar()\n",
    "plt.title(title)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Print the min and max values excluding NaNs to understand the color range\n",
    "min_val = np.nanmin(channel_data)\n",
    "max_val = np.nanmax(channel_data)\n",
    "print(f\"Data range for displayed channel: {min_val} to {max_val}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Original\n",
    "# Normalization functions\n",
    "#def normalize_data(data):\n",
    "#    min_val = np.nanmin(data, axis=(0, 1), keepdims=True)\n",
    "#    max_val = np.nanmax(data, axis=(0, 1), keepdims=True)\n",
    "#    normalized_data = (((data - min_val) / (max_val - min_val))*2)-1\n",
    "#    return normalized_data, min_val, max_val\n",
    "\n",
    "#def denormalize_data(data, min_val, max_val):\n",
    "#    return data * (max_val - min_val) + min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(data):\n",
    "    # Create a mask for -2 values\n",
    "    mask = (data != -2) & ~np.isnan(data)\n",
    "    \n",
    "    # Compute min and max excluding -2 values, along the third axis (channels)\n",
    "    with np.errstate(all='ignore'):  # Suppress warnings for NaN slices\n",
    "        min_val = np.nanmin(np.where(mask, data, np.nan), axis=(0, 1), keepdims=True)\n",
    "        max_val = np.nanmax(np.where(mask, data, np.nan), axis=(0, 1), keepdims=True)\n",
    "    \n",
    "    # Handle cases where the min and max are NaN (i.e., no valid data points in the channel)\n",
    "    min_val = np.nan_to_num(min_val, nan=0.0)  # Replace NaN with 0.0\n",
    "    max_val = np.nan_to_num(max_val, nan=1.0)  # Replace NaN with 1.0\n",
    "    \n",
    "    # Ensure min_val != max_val to avoid division by zero\n",
    "    scale = np.where(max_val != min_val, max_val - min_val, 1)\n",
    "    \n",
    "    # Normalize data excluding -2 values\n",
    "    normalized_data = np.where(mask, (((data - min_val) / scale) * 2) - 1, -2)\n",
    "    \n",
    "    return normalized_data, min_val, max_val\n",
    "\n",
    "def denormalize_data(data, min_val, max_val):\n",
    "    # Ensure min_val and max_val are numpy arrays\n",
    "    if isinstance(min_val, torch.Tensor):\n",
    "        min_val = min_val.cpu().numpy()\n",
    "    if isinstance(max_val, torch.Tensor):\n",
    "        max_val = max_val.cpu().numpy()\n",
    "    \n",
    "    # Handle NaN values in min and max\n",
    "    min_val = np.nan_to_num(min_val, nan=0.0)\n",
    "    max_val = np.nan_to_num(max_val, nan=1.0)\n",
    "\n",
    "    # Ensure min_val and max_val are 3D for broadcasting (1, channels, 1, 1)\n",
    "    min_val = min_val.reshape(1, -1, 1, 1)\n",
    "    max_val = max_val.reshape(1, -1, 1, 1)\n",
    "\n",
    "    # Create a mask for -2 values\n",
    "    mask = (data != -2) & ~np.isnan(data)\n",
    "\n",
    "    # Ensure min_val != max_val to avoid division by zero\n",
    "    scale = np.where(max_val != min_val, max_val - min_val, 1)\n",
    "\n",
    "    # Denormalize data excluding -2 values\n",
    "    denormalized_data = np.where(mask, (data + 1) / 2 * scale + min_val, -2)\n",
    "\n",
    "    return denormalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_grid_array, min_val, max_val = normalize_data(grid_array)\n",
    "print(min_val, max_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Train, Validation, and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_data = normalized_grid_array[:,:,channel_to_display]\n",
    "# Mask the -1 values by setting them to NaN\n",
    "channel_data = np.where(channel_data == 0, np.nan, channel_data)\n",
    "plt.imshow(channel_data, cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.title(f\"Normalised {column_name} with test are marked\")\n",
    "rectangle = plt.Rectangle((rect_x_start, rect_y_start), \n",
    "                          rect_x_end - rect_x_start, \n",
    "                          rect_y_end - rect_y_start, \n",
    "                          edgecolor='red', \n",
    "                          facecolor='none', \n",
    "                          linewidth=2)\n",
    "plt.gca().add_patch(rectangle)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = normalized_grid_array[rect_y_start:rect_y_end,rect_x_start:rect_x_end,:].copy()\n",
    "normalized_grid_array[rect_y_start:rect_y_end,rect_x_start:rect_x_end,:] = -3\n",
    "train_data = normalized_grid_array\n",
    "plt.imshow(train_data[:,:,channel_to_display], cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.title(f\"Test data for {channel_to_display}: {column_name} blacked out in training data\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_data[:,:,channel_to_display], cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.title(f\"Close up on for {column_name} test data\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_output_pairs(data, patch_size=patch_size, stride=patch_stride):\n",
    "    patches = []\n",
    "    for i in range(0,data.shape[0]-patch_size,stride):\n",
    "        for j in range(0,data.shape[1]-patch_size,stride):\n",
    "            current_patch = data[i:patch_size+i,j:patch_size+j].copy()\n",
    "            if not np.any(current_patch == -3): #Make sure to not include any test data by removing any patch containing -3\n",
    "                if not np.all(current_patch == -2): #Remove all patches where no data is recorded\n",
    "                    patches.append(current_patch)\n",
    "    return np.array(patches)\n",
    "\n",
    "train_patches = create_input_output_pairs(train_data)\n",
    "test_patches = create_input_output_pairs(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation\n",
    "train_patches, val_patches = train_test_split(train_patches, test_size=test_size, random_state=random_state)\n",
    "\n",
    "print(\"Train inputs shape:\", train_patches.shape)\n",
    "print(\"Val inputs shape:\", val_patches.shape)\n",
    "print(\"Test inputs shape:\", test_patches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchDataset(Dataset):\n",
    "    def __init__(self, data, mask_pixels=mask_count):\n",
    "        self.data = data\n",
    "        self.mask_pixels = mask_pixels\n",
    "        self.height, self.width = data.shape[1], data.shape[2]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]\n",
    "        image = torch.tensor(image, dtype=torch.float32)  # Convert to torch tensor\n",
    "\n",
    "        mask = torch.ones_like(image)\n",
    "        num_pixels_to_mask = self.mask_pixels\n",
    "\n",
    "        # Start from a random initial position\n",
    "        i = np.random.randint(0, self.height)\n",
    "        j = np.random.randint(0, self.width)\n",
    "        \n",
    "        # Directions for movement: up, down, left, right\n",
    "        directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "        \n",
    "        # Queue for BFS\n",
    "        queue = deque([(i, j)])\n",
    "        mask[i, j, :] = 0\n",
    "        num_pixels_to_mask -= 1\n",
    "\n",
    "        while queue and num_pixels_to_mask > 0:\n",
    "            ci, cj = queue.popleft()\n",
    "\n",
    "            for di, dj in directions:\n",
    "                ni, nj = ci + di, cj + dj\n",
    "                if 0 <= ni < self.height and 0 <= nj < self.width and mask[ni, nj, 0] == 1:\n",
    "                    mask[ni, nj, :] = 0\n",
    "                    num_pixels_to_mask -= 1\n",
    "                    queue.append((ni, nj))\n",
    "\n",
    "                    if num_pixels_to_mask == 0:\n",
    "                        break\n",
    "\n",
    "        # Ensure exactly self.mask_pixels are masked\n",
    "        if num_pixels_to_mask > 0:\n",
    "            for i in range(self.height):\n",
    "                for j in range(self.width):\n",
    "                    if num_pixels_to_mask == 0:\n",
    "                        break\n",
    "                    if mask[i, j, 0] == 1:\n",
    "                        mask[i, j, :] = 0\n",
    "                        num_pixels_to_mask -= 1\n",
    "\n",
    "        masked_image = image * mask\n",
    "\n",
    "        return masked_image.permute(2, 0, 1), image.permute(2, 0, 1), mask.permute(2, 0, 1)\n",
    "\n",
    "\n",
    "train_dataset = PatchDataset(train_patches)\n",
    "val_dataset = PatchDataset(val_patches)\n",
    "test_dataset = PatchDataset(test_patches)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Test the dataloader\n",
    "for masked_image, image, mask in train_dataloader:\n",
    "    print(masked_image.shape)\n",
    "    print(image.shape)\n",
    "    print(mask.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The forward process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_beta_schedule(timesteps, start=0.0001, end=0.02):\n",
    "    return torch.linspace(start, end, timesteps)\n",
    "\n",
    "def cosine_beta_schedule(timesteps:int, s:float=0.008)->torch.tensor:\n",
    "    \"\"\"\n",
    "    The cosine scheduler\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    timesteps : int\n",
    "        The number of timesteps\n",
    "    Returns\n",
    "    ---------\n",
    "    torch.tensor: The beta values\n",
    "\n",
    "    \"\"\"\n",
    "    def f(t):\n",
    "        return torch.cos((t / timesteps + s) / (1 + s) * 0.5 * torch.pi) ** 2\n",
    "    x = torch.linspace(0, timesteps, timesteps + 1).to(device)\n",
    "    alphas_cumprod = f(x) / f(torch.tensor([0]).to(device))\n",
    "    betas = 1 - alphas_cumprod[1:] / alphas_cumprod[:-1]\n",
    "    betas = torch.clip(betas, 0.0001, 0.999).to(device)\n",
    "    return betas\n",
    "\n",
    "def get_index_from_list(vals, t, x_shape):\n",
    "    \"\"\" \n",
    "    Returns a specific index t of a passed list of values vals\n",
    "    while considering the batch dimension.\n",
    "    \"\"\"\n",
    "    batch_size = t.shape[0]\n",
    "    out = vals.gather(-1, t).to(t.device)\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device) # Ensure output is on the same device as t\n",
    "\n",
    "def forward_diffusion_sample(x_0, t, device=device):\n",
    "    \"\"\" \n",
    "    Takes an image and a timestep as input and \n",
    "    returns the noisy version of it\n",
    "    \"\"\"\n",
    "    x_0 = x_0.to(device)\n",
    "    noise = torch.randn_like(x_0, device=device)\n",
    "    sqrt_alphas_cumprod_t = get_index_from_list(sqrt_alphas_cumprod, t, x_0.shape).to(device)\n",
    "    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x_0.shape\n",
    "    ).to(device)\n",
    "    # mean + variance\n",
    "    c1 = sqrt_alphas_cumprod_t.to(device) * x_0 \n",
    "    c2 = c1 + sqrt_one_minus_alphas_cumprod_t.to(device) * noise \n",
    "    return c2, noise.to(device) \n",
    "\n",
    "\n",
    "# Define beta schedule\n",
    "\n",
    "betas = cosine_beta_schedule(timesteps=T)\n",
    "\n",
    "# Pre-calculate different terms for closed form\n",
    "alphas = 1. - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n",
    "posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
    "\n",
    "# Debugging: Print and visualize the betas and alphas\n",
    "print(\"Betas:\", betas)\n",
    "print(\"Alphas:\", alphas)\n",
    "print(\"Alphas Cumprod:\", alphas_cumprod)\n",
    "\n",
    "# Plot betas\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(betas.cpu().numpy(), label='Betas')\n",
    "plt.plot(alphas.cpu().numpy(), label='Alphas')\n",
    "plt.title('Beta and Alpha Schedule')\n",
    "plt.xlabel('Timestep')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot alphas cumprod\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(alphas_cumprod.cpu().numpy(), label='Alphas Cumprod')\n",
    "plt.title('Alphas Cumprod Schedule')\n",
    "plt.xlabel('Timestep')\n",
    "plt.ylabel('Alpha Cumprod Value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate forward diffusion\n",
    "image_masked,image,mask = next(iter(train_dataloader))\n",
    "plt.figure(figsize=(15,15))\n",
    "stepsize = int(T/num_images)\n",
    "\n",
    "def show_tensor_image(image):\n",
    "    # Take first image of batch\n",
    "    if len(image.shape) == 4:\n",
    "        image = image[0, channel_to_display:channel_to_display+1, :, :] \n",
    "    plt.imshow(image.permute(1, 2, 0), cmap='gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "\n",
    "for idx in range(0, T, stepsize):\n",
    "    t = torch.Tensor([idx]).type(torch.int64).to(device)\n",
    "    plt.subplot(1, num_images+1, int(idx/stepsize) + 1)\n",
    "    img, noise = forward_diffusion_sample(image, t)\n",
    "    show_tensor_image(img.detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(train_dataloader))\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Show the first image on the first subplot\n",
    "plt.sca(axes[0])\n",
    "show_tensor_image(sample[1])\n",
    "axes[0].set_title('Original Image')\n",
    "\n",
    "# Show the second image on the second subplot\n",
    "plt.sca(axes[1])\n",
    "show_tensor_image(sample[0])\n",
    "axes[1].set_title('Masked Image')\n",
    "\n",
    "# Display the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_tensor_image(sample[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The backward process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim, up=False):\n",
    "        super().__init__()\n",
    "        self.time_mlp =  nn.Linear(time_emb_dim, out_ch).to(device)\n",
    "        if up:\n",
    "            self.conv1 = nn.Conv2d(2*in_ch, out_ch, 3, padding=1).to(device)\n",
    "            self.transform = nn.ConvTranspose2d(out_ch, out_ch, 4, 2, 1).to(device)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1).to(device)\n",
    "            self.transform = nn.Conv2d(out_ch, out_ch, 4, 2, 1).to(device)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1).to(device)\n",
    "        self.bnorm1 = nn.BatchNorm2d(out_ch).to(device)\n",
    "        self.bnorm2 = nn.BatchNorm2d(out_ch).to(device)\n",
    "        self.relu  = nn.ReLU().to(device)\n",
    "        \n",
    "    def forward(self, x, t, ):\n",
    "        # First Conv\n",
    "        h = self.bnorm1(self.relu(self.conv1(x)))\n",
    "        # Time embedding\n",
    "        time_emb = self.relu(self.time_mlp(t))\n",
    "        # Extend last 2 dimensions\n",
    "        time_emb = time_emb[(..., ) + (None, ) * 2]\n",
    "        # Add time channel\n",
    "        h = h + time_emb\n",
    "        # Second Conv\n",
    "        h = self.bnorm2(self.relu(self.conv2(h)))\n",
    "        # Down or Upsample\n",
    "        return self.transform(h)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim, up=False, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.time_mlp = nn.Linear(time_emb_dim, out_ch).to(device)\n",
    "        if up:\n",
    "            self.conv1 = nn.Conv2d(2 * in_ch, out_ch, 3, padding=1).to(device)\n",
    "            self.transform = nn.ConvTranspose2d(out_ch, out_ch, 4, 2, 1).to(device)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1).to(device)\n",
    "            self.transform = nn.Conv2d(out_ch, out_ch, 4, 2, 1).to(device)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1).to(device)\n",
    "        self.bnorm1 = nn.BatchNorm2d(out_ch).to(device)\n",
    "        self.bnorm2 = nn.BatchNorm2d(out_ch).to(device)\n",
    "        self.relu = nn.ReLU().to(device)\n",
    "        \n",
    "        # Define the attention layer\n",
    "        self.attention = MultiheadAttention(embed_dim=out_ch, num_heads=num_heads).to(device)\n",
    "        self.attention_norm = nn.LayerNorm(out_ch).to(device)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        # First Conv\n",
    "        h = self.bnorm1(self.relu(self.conv1(x)))\n",
    "        \n",
    "        # Time embedding\n",
    "        time_emb = self.relu(self.time_mlp(t))\n",
    "        \n",
    "        # Extend last 2 dimensions\n",
    "        time_emb = time_emb[(..., ) + (None, ) * 2]\n",
    "        \n",
    "        # Add time channel\n",
    "        h = h + time_emb\n",
    "        \n",
    "        # Apply attention\n",
    "        B, C, H, W = h.shape\n",
    "        h_flat = h.view(B, C, -1).permute(2, 0, 1)  # (H*W, B, C)\n",
    "        attn_output, _ = self.attention(h_flat, h_flat, h_flat)\n",
    "        attn_output = attn_output.permute(1, 2, 0).view(B, C, H, W)\n",
    "        h = self.attention_norm(h + attn_output)\n",
    "        \n",
    "        # Second Conv\n",
    "        h = self.bnorm2(self.relu(self.conv2(h)))\n",
    "        \n",
    "        # Down or Upsample\n",
    "        return self.transform(h)\n",
    "\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        # TODO: Double check the ordering here\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class SimpleUnet(nn.Module):\n",
    "    \"\"\"\n",
    "    A simplified variant of the Unet architecture.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        image_channels = channels\n",
    "        down_channels = (patch_size, patch_size * 2, patch_size * 4, patch_size * 8, patch_size * 16)\n",
    "        up_channels = (patch_size * 16, patch_size * 8, patch_size * 4, patch_size * 2, patch_size)\n",
    "        out_dim = channels\n",
    "        time_emb_dim = 32\n",
    "\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(time_emb_dim).to(device),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim).to(device),\n",
    "            nn.ReLU().to(device)\n",
    "        )\n",
    "\n",
    "        self.conv0 = nn.Conv2d(image_channels, down_channels[0], 3, padding=1).to(device)\n",
    "\n",
    "        self.downs = nn.ModuleList([Block(down_channels[i], down_channels[i + 1], \\\n",
    "                                          time_emb_dim).to(device) \\\n",
    "                    for i in range(len(down_channels) - 1)])\n",
    "    \n",
    "        self.ups = nn.ModuleList([Block(up_channels[i], up_channels[i + 1], \\\n",
    "                                        time_emb_dim, up=True).to(device) \\\n",
    "                    for i in range(len(up_channels) - 1)])\n",
    "        \n",
    "        self.output = nn.Conv2d(up_channels[-1], out_dim, 1).to(device)\n",
    "\n",
    "    def forward(self, x, timestep):\n",
    "        t = self.time_mlp(timestep)\n",
    "        x = self.conv0(x)\n",
    "        residual_inputs = []\n",
    "        for down in self.downs:\n",
    "            x = down(x, t)\n",
    "            residual_inputs.append(x)\n",
    "        for up in self.ups:\n",
    "            residual_x = residual_inputs.pop()\n",
    "            if x.size(2) != residual_x.size(2) or x.size(3) != residual_x.size(3):\n",
    "                x = F.interpolate(x, size=(residual_x.size(2), residual_x.size(3)), mode='bilinear', align_corners=False)\n",
    "            x = torch.cat((x, residual_x), dim=1)\n",
    "            x = up(x, t)\n",
    "        return self.output(x)\n",
    "\n",
    "model = SimpleUnet().to(device)\n",
    "print(\"Num params: \", sum(p.numel() for p in model.parameters()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def get_loss(model, x_0, t):\n",
    "#    x_noisy, noise = forward_diffusion_sample(x_0, t, device)\n",
    "#    noise_pred = model(x_noisy, t)\n",
    "#    return F.mse_loss(noise_pred,noise)\n",
    "    #return F.l1_loss(noise, noise_pred)\n",
    "\n",
    "def get_loss(model, x_0, t):\n",
    "    # Apply the forward diffusion process to generate noisy data\n",
    "    x_noisy, noise = forward_diffusion_sample(x_0, t, device)\n",
    "    \n",
    "    # Predict the noise from the noisy data using the model\n",
    "    noise_pred = model(x_noisy, t)\n",
    "    \n",
    "    # Create a mask where True represents values not equal to -2\n",
    "    mask = (x_0 != -2) & ~torch.isnan(x_0)\n",
    "    \n",
    "    # Apply the mask to filter out -2 values from noise and noise_pred\n",
    "    valid_noise = noise[mask]\n",
    "    valid_noise_pred = noise_pred[mask]\n",
    "    \n",
    "    # Compute the mean squared error loss only for valid values\n",
    "    loss = F.mse_loss(valid_noise_pred, valid_noise)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_timestep(x, t):\n",
    "    \"\"\"\n",
    "    Calls the model to predict the noise in the image and returns \n",
    "    the denoised image. \n",
    "    Applies noise to this image, if we are not in the last step yet.\n",
    "    \"\"\"\n",
    "    betas_t = get_index_from_list(betas, t, x.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x.shape\n",
    "    )\n",
    "    sqrt_recip_alphas_t = get_index_from_list(sqrt_recip_alphas, t, x.shape)\n",
    "    \n",
    "    # Call model (current image - noise prediction)\n",
    "    model_mean = sqrt_recip_alphas_t * (\n",
    "        x - betas_t * model(x, t) / sqrt_one_minus_alphas_cumprod_t\n",
    "    )\n",
    "    posterior_variance_t = get_index_from_list(posterior_variance, t, x.shape)\n",
    "    \n",
    "    if t == 0:\n",
    "        return model_mean \n",
    "    else:\n",
    "        noise = torch.randn_like(x)#, device=device)\n",
    "        return model_mean + torch.sqrt(posterior_variance_t) * noise #\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def inpainting_plot_image(image, mask):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    stepsize = int(T / num_images)\n",
    "\n",
    "    # Display the original image\n",
    "    plt.subplot(1, num_images + 1, 1)\n",
    "    show_tensor_image(image.detach().cpu())\n",
    "    plt.title(f\"Original Image\")  # Set the title of the subplot\n",
    "    \n",
    "    # Display the initial masked image\n",
    "    masked_image = image * mask\n",
    "\n",
    "    image_list = []\n",
    "    image_idx_plot_list = []\n",
    "\n",
    "    # Forward diffusion process\n",
    "    for idx in range(0, T):  # Iterate over all timesteps\n",
    "        t = torch.Tensor([idx]).type(torch.int64).to(device)  # Convert idx to tensor\n",
    "        img, noise = forward_diffusion_sample(masked_image, t, device)  # Apply forward diffusion to generate noisy image\n",
    "        if idx % stepsize == 0:  # Plot the image if the timestep is in the list of timesteps to plot\n",
    "            image_idx_plot_list.append(idx)  # Append the timestep index to the list\n",
    "            plt.subplot(1, num_images + 1, int(idx / stepsize) + 2)  # Create a subplot for the image\n",
    "            show_tensor_image(img.detach().cpu())  # Show the noisy image at this timestep\n",
    "            plt.title(f\"Timestep {idx}\")  # Set the title of the subplot\n",
    "        image_list.append((img, t))  # Save the noisy image and its timestep\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Backward denoising process\n",
    "    plt.figure(figsize=(15, 15))  # Prepare to plot the denoising process\n",
    "    plt.subplot(1, num_images + 1, 1)  # Create a subplot for the masked image\n",
    "    tmp_mask = torch.ones_like(mask)  # Initialize the mask to 1\n",
    "    tmp_img = image_list[-1][0]  # Start with the last noisy image from the forward process\n",
    "\n",
    "    for idx, (noisy_img, t) in enumerate(image_list[::-1]):  # Iterate over the noisy images in reverse order\n",
    "        # Extract the single element from the tensor\n",
    "        t_idx = int(t.item())\n",
    "\n",
    "        # Predict the denoised image at this timestep\n",
    "        tmp_img = sample_timestep(noisy_img, t)  # Predict the denoised image at this timestep\n",
    "\n",
    "        # Combine the noisy image with the progressively denoised image using the mask\n",
    "        img = (noisy_img * tmp_mask) + (tmp_img * (1 - tmp_mask))\n",
    "\n",
    "        # Update the tmp_mask to mask so that after the first step only the originally masked part in the forward process is used\n",
    "        tmp_mask = mask\n",
    "\n",
    "        # Debug prints for denoising process\n",
    "        print(f\"Backward process - Step {idx}, Timestep {t_idx}\")\n",
    "        print(f\"noisy_img min: {noisy_img.min().item()}, max: {noisy_img.max().item()}\")\n",
    "        print(f\"tmp_img min: {tmp_img.min().item()}, max: {tmp_img.max().item()}\")\n",
    "        print(f\"combined_img min: {img.min().item()}, max: {img.max().item()}\")\n",
    "\n",
    "        # Plot the image if the timestep is in the list of timesteps to plot\n",
    "        if t_idx in image_idx_plot_list:\n",
    "            subplot_index = num_images - (image_idx_plot_list.index(t_idx))\n",
    "            print(f\"INSIDE LOOP: Timestep {t_idx}\")\n",
    "            print(f\"original idx: {subplot_index}\")\n",
    "            plt.subplot(1, num_images + 1, subplot_index + 2)  # Correct subplot index for denoised image\n",
    "            show_tensor_image(img.detach().cpu())  # Show denoised image for that step\n",
    "            plt.title(f\"Timestep {t_idx}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    #plt.figure(figsize=(15, 15))\n",
    "    #plt.subplot(1, num_images, 1)\n",
    "    #tmp_mask = 1\n",
    "    #tmp_img = image_list[-1][0]\n",
    "    #for idx, (noisy_img,t) in enumerate(image_list[::-1]):\n",
    "        #img = (noisy_img*tmp_mask)+(tmp_img*(1-tmp_mask))\n",
    "        #tmp_mask = mask\n",
    "        #tmp_img = sample_timestep(img,t)\n",
    "        #if t in image_idx_plot_list:\n",
    "            #plt.subplot(1, num_images, num_images-(int(idx/stepsize)))\n",
    "            #show_tensor_image(tmp_img.detach().cpu())\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "def train_epoch(model):\n",
    "    loss_value = 0\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        t = torch.randint(0, T, (batch[1].shape[0],), device=device).long()\n",
    "        loss = get_loss(model, batch[1].to(device), t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_value+=loss.item()\n",
    "    return loss_value/len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def val_epoch(model):\n",
    "    loss_value = 0\n",
    "    for batch in val_dataloader:\n",
    "        t = torch.randint(0, T, (batch[1].shape[0],), device=device).long()\n",
    "        loss = get_loss(model, batch[1].to(device), t)\n",
    "        loss_value+=loss.item()\n",
    "    return loss_value/len(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "loss_list = []\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_epoch(model)\n",
    "    val_loss = val_epoch(model)\n",
    "    print(f\"Epoch {epoch}/{epochs}: Train Loss: {train_loss}, Validation Loss: {val_loss}\")\n",
    "    loss_list.append((train_loss,val_loss))\n",
    "    #if epoch > 0:\n",
    "    #    sample = next(iter(train_dataloader))\n",
    "    #    inpainting_plot_image(sample[1].to(device), sample[2].to(device))\n",
    "        #inpainting_plot_image(sample[1].to(device),sample[2].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'best_model_mapillaryFeatures.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "# Plot the training loss\n",
    "plt.plot([g[0] for g in loss_list], label='Train Loss')\n",
    "\n",
    "# Plot the validation loss\n",
    "plt.plot([g[1] for g in loss_list], color='orange', label='Validation Loss')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "from torchviz import make_dot\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a wrapper function\n",
    "def model_summary_wrapper(model, input_size):\n",
    "    class ModelWrapper(nn.Module):\n",
    "        def __init__(self, model):\n",
    "            super(ModelWrapper, self).__init__()\n",
    "            self.model = model\n",
    "        \n",
    "        def forward(self, x):\n",
    "            # Create a dummy timestep tensor\n",
    "            batch_size = x.size(0)\n",
    "            timestep = torch.zeros(batch_size, dtype=torch.long, device=x.device)\n",
    "            return self.model(x, timestep)\n",
    "    \n",
    "    return ModelWrapper(model)\n",
    "\n",
    "# Wrap your model\n",
    "wrapped_model = model_summary_wrapper(model, (channels, patch_size, patch_size))\n",
    "\n",
    "# Print the model summary\n",
    "summary(wrapped_model, (channels, patch_size, patch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = SimpleUnet().to(device)\n",
    "\n",
    "# Load the saved model state\n",
    "model.load_state_dict(torch.load('best_model_mapillaryFeatures.pt'))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function to evaluate the model's performance on the test dataset\n",
    "def test_model(model, dataloader, min_val, max_val, num_examples=3, channels=51):\n",
    "    model.eval()\n",
    "    mse_per_channel_normalized = torch.zeros(channels, dtype=torch.float32).to(device)\n",
    "    mse_per_channel_denormalized = torch.zeros(channels, dtype=torch.float32).to(device)\n",
    "    count_per_channel = torch.zeros(channels, dtype=torch.float32).to(device)\n",
    "    examples_printed = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        masked_image, original_image, mask = batch\n",
    "        masked_image = masked_image.to(device)\n",
    "        original_image = original_image.to(device)\n",
    "        mask = mask.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            t = torch.randint(0, T, (masked_image.shape[0],), device=device).long()\n",
    "            generated_image = model(masked_image, t)\n",
    "\n",
    "        # Convert tensors to CPU and NumPy arrays for denormalization\n",
    "        original_image_np = original_image.cpu().numpy()\n",
    "        generated_image_np = generated_image.cpu().numpy()\n",
    "\n",
    "        # Denormalize the images\n",
    "        denormalized_original = denormalize_data(original_image_np, min_val, max_val)\n",
    "        denormalized_generated = denormalize_data(generated_image_np, min_val, max_val)\n",
    "\n",
    "        if examples_printed < num_examples:\n",
    "            original_means = []\n",
    "            generated_means = []\n",
    "            denormalized_original_means = []\n",
    "            denormalized_generated_means = []\n",
    "            for channel in range(channels):\n",
    "                valid_mask = (mask[:, channel, :, :] == 0)\n",
    "                if valid_mask.any():\n",
    "                    valid_data_mask = (original_image[:, channel, :, :] != -2)\n",
    "                    valid_combined_mask = valid_mask & valid_data_mask\n",
    "\n",
    "                    original_channel_mean = torch.mean(original_image[:, channel, :, :][valid_combined_mask]).item()\n",
    "                    generated_channel_mean = torch.mean(generated_image[:, channel, :, :][valid_combined_mask]).item()\n",
    "\n",
    "                    denormalized_original_channel_mean = np.mean(denormalized_original[:, channel, :, :][valid_combined_mask.cpu().numpy()])\n",
    "                    denormalized_generated_channel_mean = np.mean(denormalized_generated[:, channel, :, :][valid_combined_mask.cpu().numpy()])\n",
    "                else:\n",
    "                    original_channel_mean = float('nan')\n",
    "                    generated_channel_mean = float('nan')\n",
    "                    denormalized_original_channel_mean = float('nan')\n",
    "                    denormalized_generated_channel_mean = float('nan')\n",
    "                original_means.append(original_channel_mean)\n",
    "                generated_means.append(generated_channel_mean)\n",
    "                denormalized_original_means.append(denormalized_original_channel_mean)\n",
    "                denormalized_generated_means.append(denormalized_generated_channel_mean)\n",
    "\n",
    "            print(f\"Example {examples_printed + 1}:\")\n",
    "            print(\"Original masked means (ignoring -2):\", [round(mean, 3) for mean in original_means])\n",
    "            print(\"Generated means:\", [round(mean, 3) for mean in generated_means])\n",
    "            print(\"Denormalized original means:\", [round(mean, 3) for mean in denormalized_original_means])\n",
    "            print(\"Denormalized generated means:\", [round(mean, 3) for mean in denormalized_generated_means])\n",
    "            print()\n",
    "\n",
    "            examples_printed += 1\n",
    "\n",
    "        for channel in range(channels):\n",
    "            valid_mask = (mask[:, channel, :, :] == 0)\n",
    "            if valid_mask.any():\n",
    "                valid_data_mask = (original_image[:, channel, :, :] != -2)\n",
    "                valid_combined_mask = valid_mask & valid_data_mask\n",
    "\n",
    "                channel_mse_normalized = F.mse_loss(generated_image[:, channel, :, :][valid_combined_mask], original_image[:, channel, :, :][valid_combined_mask], reduction='mean')\n",
    "                mse_per_channel_normalized[channel] += channel_mse_normalized * valid_combined_mask.sum()\n",
    "\n",
    "                denorm_generated_tensor = torch.tensor(denormalized_generated[:, channel, :, :][valid_combined_mask.cpu().numpy()], dtype=torch.float32)\n",
    "                denorm_original_tensor = torch.tensor(denormalized_original[:, channel, :, :][valid_combined_mask.cpu().numpy()], dtype=torch.float32)\n",
    "                denorm_mask = torch.isnan(denorm_generated_tensor) | torch.isnan(denorm_original_tensor)\n",
    "\n",
    "                if not denorm_mask.all():\n",
    "                    channel_mse_denormalized = F.mse_loss(\n",
    "                        denorm_generated_tensor[~denorm_mask],\n",
    "                        denorm_original_tensor[~denorm_mask],\n",
    "                        reduction='mean'\n",
    "                    )\n",
    "                    mse_per_channel_denormalized[channel] += channel_mse_denormalized * (~denorm_mask).sum()\n",
    "\n",
    "                count_per_channel[channel] += valid_combined_mask.sum()\n",
    "\n",
    "    average_mse_per_channel_normalized = mse_per_channel_normalized / count_per_channel\n",
    "    average_mse_per_channel_denormalized = mse_per_channel_denormalized / count_per_channel\n",
    "\n",
    "    return average_mse_per_channel_normalized, average_mse_per_channel_denormalized\n",
    "\n",
    "\n",
    "# Test the model\n",
    "average_mse_per_channel_normalized, average_mse_per_channel_denormalized = test_model(model, test_dataloader, min_val, max_val, channels=51)\n",
    "print(\"Average MSE per channel (Normalized):\", [round(mse.item(), 3) for mse in average_mse_per_channel_normalized])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, dataloader, min_val, max_val, num_examples=3, channels=26):\n",
    "    model.eval()\n",
    "    mse_per_channel_normalized = torch.zeros(channels, dtype=torch.float32).to(device)\n",
    "    mse_per_channel_denormalized = torch.zeros(channels, dtype=torch.float32).to(device)\n",
    "    count_per_channel = torch.zeros(channels, dtype=torch.float32).to(device)\n",
    "    examples_printed = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        masked_image, original_image, mask = batch\n",
    "        masked_image = masked_image.to(device)\n",
    "        original_image = original_image.to(device)\n",
    "        mask = mask.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            t = torch.randint(0, T, (masked_image.shape[0],), device=device).long()\n",
    "            generated_image = model(masked_image, t)\n",
    "\n",
    "        # Convert tensors to CPU and NumPy arrays for denormalization\n",
    "        original_image_np = original_image.cpu().numpy()\n",
    "        generated_image_np = generated_image.cpu().numpy()\n",
    "\n",
    "        # Denormalize the images\n",
    "        denormalized_original = denormalize_data(original_image_np, min_val, max_val)\n",
    "        denormalized_generated = denormalize_data(generated_image_np, min_val, max_val)\n",
    "\n",
    "        if examples_printed < num_examples:\n",
    "            original_means = []\n",
    "            generated_means = []\n",
    "            denormalized_original_means = []\n",
    "            denormalized_generated_means = []\n",
    "            for channel in range(channels):\n",
    "                valid_mask = (mask[:, channel, :, :] == 0)\n",
    "                if valid_mask.any():\n",
    "                    valid_data_mask = (original_image[:, channel, :, :] != -2)\n",
    "                    valid_combined_mask = valid_mask & valid_data_mask\n",
    "\n",
    "                    original_channel_mean = torch.mean(original_image[:, channel, :, :][valid_combined_mask]).item()\n",
    "                    generated_channel_mean = torch.mean(generated_image[:, channel, :, :][valid_combined_mask]).item()\n",
    "\n",
    "                    denormalized_original_channel_mean = np.mean(denormalized_original[:, channel, :, :][valid_combined_mask.cpu().numpy()])\n",
    "                    denormalized_generated_channel_mean = np.mean(denormalized_generated[:, channel, :, :][valid_combined_mask.cpu().numpy()])\n",
    "                else:\n",
    "                    original_channel_mean = float('nan')\n",
    "                    generated_channel_mean = float('nan')\n",
    "                    denormalized_original_channel_mean = float('nan')\n",
    "                    denormalized_generated_channel_mean = float('nan')\n",
    "                original_means.append(original_channel_mean)\n",
    "                generated_means.append(generated_channel_mean)\n",
    "                denormalized_original_means.append(denormalized_original_channel_mean)\n",
    "                denormalized_generated_means.append(denormalized_generated_channel_mean)\n",
    "\n",
    "            print(f\"Example {examples_printed + 1}:\")\n",
    "            print(\"Original masked means (ignoring -2):\", [round(mean, 3) for mean in original_means])\n",
    "            print(\"Generated means:\", [round(mean, 3) for mean in generated_means])\n",
    "            print(\"Denormalized original means:\", [round(mean, 3) for mean in denormalized_original_means])\n",
    "            print(\"Denormalized generated means:\", [round(mean, 3) for mean in denormalized_generated_means])\n",
    "            print()\n",
    "\n",
    "            examples_printed += 1\n",
    "\n",
    "        for channel in range(channels):\n",
    "            valid_mask = (mask[:, channel, :, :] == 0)\n",
    "            if valid_mask.any():\n",
    "                valid_data_mask = (original_image[:, channel, :, :] != -2)\n",
    "                valid_combined_mask = valid_mask & valid_data_mask\n",
    "\n",
    "                # Calculate the normalized MSE\n",
    "                channel_mse_normalized = F.mse_loss(\n",
    "                    generated_image[:, channel, :, :][valid_combined_mask], \n",
    "                    original_image[:, channel, :, :][valid_combined_mask], \n",
    "                    reduction='sum'\n",
    "                )\n",
    "                mse_per_channel_normalized[channel] += channel_mse_normalized\n",
    "                count_per_channel[channel] += valid_combined_mask.sum().item()\n",
    "\n",
    "                # Calculate the denormalized MSE\n",
    "                denorm_generated_tensor = torch.tensor(\n",
    "                    denormalized_generated[:, channel, :, :][valid_combined_mask.cpu().numpy()], \n",
    "                    dtype=torch.float32\n",
    "                )\n",
    "                denorm_original_tensor = torch.tensor(\n",
    "                    denormalized_original[:, channel, :, :][valid_combined_mask.cpu().numpy()], \n",
    "                    dtype=torch.float32\n",
    "                )\n",
    "                denorm_mask = torch.isnan(denorm_generated_tensor) | torch.isnan(denorm_original_tensor)\n",
    "\n",
    "                if not denorm_mask.all():\n",
    "                    channel_mse_denormalized = F.mse_loss(\n",
    "                        denorm_generated_tensor[~denorm_mask],\n",
    "                        denorm_original_tensor[~denorm_mask],\n",
    "                        reduction='sum'\n",
    "                    )\n",
    "                    mse_per_channel_denormalized[channel] += channel_mse_denormalized.item()\n",
    "\n",
    "    average_mse_per_channel = mse_per_channel_normalized# / count_per_channel\n",
    "    #average_mse_per_channel_denormalized = mse_per_channel_denormalized / count_per_channel\n",
    "\n",
    "    rmse_per_channel = torch.sqrt(average_mse_per_channel)\n",
    "    rmse_per_channel_denormalized = torch.sqrt(average_mse_per_channel_denormalized)\n",
    "\n",
    "    return rmse_per_channel\n",
    "\n",
    "# Test the model\n",
    "rmse_per_channel_normalized, rmse_per_channel_denormalized = test_model(model, test_dataloader, min_val, max_val, channels=51)\n",
    "print(\"RMSE per channel (Normalized):\", [round(rmse.item(), 3) for rmse in rmse_per_channel])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_dataloader, min_val, max_val, device=\"cpu\", num_examples=3):\n",
    "    model.eval()\n",
    "    \n",
    "    original_means = []\n",
    "    generated_means = []\n",
    "    \n",
    "    # Collecting means for each channel\n",
    "    for batch in test_dataloader:\n",
    "        masked_images, original_images, masks = batch\n",
    "        masked_images = masked_images.to(device)\n",
    "        original_images = original_images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        \n",
    "        # Generate predictions\n",
    "        t = torch.randint(0, T, (masked_images.shape[0],), device=device).long()\n",
    "        predictions = model(masked_images, t)\n",
    "        \n",
    "        # Print the range of the model's output for debugging\n",
    "        print(\"Predictions range before denormalization:\", predictions.min().item(), predictions.max().item())\n",
    "        \n",
    "        # Detach tensors from the computation graph and convert to NumPy arrays\n",
    "        predictions_np = predictions.detach().cpu().numpy()\n",
    "        original_images_np = original_images.detach().cpu().numpy()\n",
    "        \n",
    "        # Denormalize the predictions and original images\n",
    "        denormalized_predictions = denormalize_data(predictions_np, min_val, max_val)\n",
    "        denormalized_originals = denormalize_data(original_images_np, min_val, max_val)\n",
    "        \n",
    "        # Calculate means\n",
    "        original_mean = np.mean(denormalized_originals, axis=(0, 2, 3))\n",
    "        generated_mean = np.mean(denormalized_predictions, axis=(0, 2, 3))\n",
    "        \n",
    "        original_means.append(original_mean)\n",
    "        generated_means.append(generated_mean)\n",
    "    \n",
    "    # Converting lists to arrays\n",
    "    original_means = np.array(original_means)\n",
    "    generated_means = np.array(generated_means)\n",
    "    \n",
    "    # Print some example means\n",
    "    print(\"Original Means of the Mask (Examples):\")\n",
    "    print(original_means[:num_examples])\n",
    "    print(\"Generated Means of the Mask (Examples):\")\n",
    "    print(generated_means[:num_examples])\n",
    "    \n",
    "    # Compute MSE for all channels individually\n",
    "    mse_per_channel = np.mean((original_means - generated_means) ** 2, axis=0)\n",
    "    \n",
    "    return mse_per_channel\n",
    "\n",
    "# Example usage\n",
    "mse_per_channel = test_model(model, test_dataloader, min_val, max_val, device)\n",
    "print(\"MSE for each channel:\", mse_per_channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                           Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.counter = 0\n",
    "        self.best_loss = np.Inf\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'checkpoint.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "epochs = 100\n",
    "num_images = 5\n",
    "channel_to_display = 6\n",
    "\n",
    "# Hyperparameter space\n",
    "hyperparameter_space = {\n",
    "    'learning_rate': [1e-3, 1e-4, 1e-5],\n",
    "    'batch_size': [4, 8, 16],\n",
    "    'mask_count': [100, 140, 180],\n",
    "    'T': [500, 1000, 1500],\n",
    "    'patch_size': [30, 40, 50],\n",
    "    'patch_stride': [10, 20, 25]\n",
    "}\n",
    "\n",
    "# Define the updated sampling function\n",
    "def sample_hyperparameters(hyperparameter_space, max_T):\n",
    "    sampled_params = {key: random.choice(values) for key, values in hyperparameter_space.items()}\n",
    "    # Ensure T is within bounds\n",
    "    sampled_params['T'] = min(sampled_params['T'], max_T)\n",
    "    return sampled_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform random search\n",
    "n_iterations = 10\n",
    "results = []\n",
    "\n",
    "max_T = T  # Set the maximum value of T based on initial value\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    print(f\"Iteration {i+1}/{n_iterations}\")\n",
    "    hyperparameters = sample_hyperparameters(hyperparameter_space, max_T)\n",
    "    \n",
    "    # Update hyperparameters\n",
    "    learning_rate = hyperparameters['learning_rate']\n",
    "    BATCH_SIZE = hyperparameters['batch_size']\n",
    "    mask_count = hyperparameters['mask_count']\n",
    "    T = hyperparameters['T']\n",
    "    patch_size = hyperparameters['patch_size']\n",
    "    patch_stride = hyperparameters['patch_stride']\n",
    "    \n",
    "    # Define the model, optimizer, and data loaders here based on the new hyperparameters\n",
    "    model = SimpleUnet().to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    train_dataset = PatchDataset(train_patches, mask_pixels=mask_count)\n",
    "    val_dataset = PatchDataset(val_patches, mask_pixels=mask_count)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    # Redefine beta schedule and precomputed terms based on the new T\n",
    "    betas = cosine_beta_schedule(timesteps=T)\n",
    "    alphas = 1. - betas\n",
    "    alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "    alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0).to(device)\n",
    "    sqrt_recip_alphas = torch.sqrt(1.0 / alphas).to(device)\n",
    "    sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod).to(device)\n",
    "    sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod).to(device)\n",
    "    posterior_variance = (betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)).to(device)\n",
    "    \n",
    "    # Train and validate the model\n",
    "    loss_list = []\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_epoch(model)\n",
    "        val_loss = val_epoch(model)\n",
    "        print(f\"Epoch {epoch}/{epochs}: Train Loss: {train_loss}, Validation Loss: {val_loss}\")\n",
    "        loss_list.append((train_loss, val_loss))\n",
    "    \n",
    "    # Log results\n",
    "    results.append({\n",
    "        'hyperparameters': hyperparameters,\n",
    "        'validation_loss': val_loss\n",
    "    })\n",
    "\n",
    "# Save the results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('random_search_results.csv', index=False)\n",
    "\n",
    "# Find the best hyperparameters\n",
    "best_result = results_df.loc[results_df['validation_loss'].idxmin()]\n",
    "print(f\"Best hyperparameters: {best_result['hyperparameters']}\")\n",
    "print(f\"Best validation loss: {best_result['validation_loss']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the final model with the best hyperparameters on the combined training and validation dataset\n",
    "best_hyperparameters = best_result['hyperparameters']\n",
    "learning_rate = best_hyperparameters['learning_rate']\n",
    "BATCH_SIZE = best_hyperparameters['batch_size']\n",
    "mask_count = best_hyperparameters['mask_count']\n",
    "T = best_hyperparameters['T']\n",
    "patch_size = best_hyperparameters['patch_size']\n",
    "patch_stride = best_hyperparameters['patch_stride']\n",
    "\n",
    "final_model = SimpleUnet().to(device)\n",
    "final_optimizer = Adam(final_model.parameters(), lr=learning_rate)\n",
    "\n",
    "combined_train_val_patches = np.concatenate((train_patches, val_patches))\n",
    "combined_train_val_dataset = PatchDataset(combined_train_val_patches, mask_pixels=mask_count)\n",
    "combined_train_val_dataloader = DataLoader(combined_train_val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_epoch(final_model)\n",
    "    print(f\"Epoch {epoch}/{epochs}: Train Loss: {train_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'best_model_HyperDone.pt')\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# Plot the training loss\n",
    "plt.plot([g[0] for g in loss_list], label='Train Loss')\n",
    "\n",
    "# Plot the validation loss\n",
    "plt.plot([g[1] for g in loss_list], color='orange', label='Validation Loss')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
